{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/data-science-london-scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"...input/train.csv\",header=None)\n",
    "trainLabels = pd.read_csv(\"...input/trainLabels.csv\",header=None)\n",
    "test = pd.read_csv(\"...input/test.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 40), (1000, 1), (9000, 40))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,trainLabels.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().any().any())\n",
    "print(test.isnull().any().any())\n",
    "print(trainLabels.isnull().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 40), (200, 40), (800, 1), (200, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(train,trainLabels,test_size=0.2,random_state=123)\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.81\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80        94\n",
      "           1       0.81      0.83      0.82       106\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.81      0.81      0.81       200\n",
      "weighted avg       0.81      0.81      0.81       200\n",
      "\n",
      "[[74 20]\n",
      " [18 88]]\n"
     ]
    }
   ],
   "source": [
    "#1.Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train,y_train.values.ravel())\n",
    "predict_gnb = gnb.predict(x_test)\n",
    "print(\"Naive Bayes:\",accuracy_score(y_test,predict_gnb))\n",
    "print(metrics.classification_report(y_test,predict_gnb))\n",
    "''''''\n",
    "print(metrics.confusion_matrix(y_test,predict_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82        94\n",
      "           1       0.83      0.87      0.85       106\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.84      0.83      0.83       200\n",
      "weighted avg       0.84      0.83      0.83       200\n",
      "\n",
      "[[75 19]\n",
      " [14 92]]\n"
     ]
    }
   ],
   "source": [
    "#2.Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train.values.ravel())\n",
    "predict_lr = lr.predict(x_test)\n",
    "print(\"Logistic Regression:\",accuracy_score(y_test,predict_lr))\n",
    "print(metrics.classification_report(y_test,predict_lr))\n",
    "''''''\n",
    "print(metrics.confusion_matrix(y_test,predict_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors: 0.91\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91        94\n",
      "           1       0.96      0.87      0.91       106\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.91      0.91       200\n",
      "weighted avg       0.91      0.91      0.91       200\n",
      "\n",
      "[[90  4]\n",
      " [14 92]]\n"
     ]
    }
   ],
   "source": [
    "#3.K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2, weights='uniform', algorithm='auto',\n",
    "                           leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn.fit(x_train,y_train.values.ravel())\n",
    "predict_knn = knn.predict(x_test)\n",
    "print(\"Nearest Neighbors:\",accuracy_score(y_test,predict_knn))\n",
    "print(metrics.classification_report(y_test,predict_knn))\n",
    "print(metrics.confusion_matrix(y_test,predict_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: 0.795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.80        94\n",
      "           1       0.86      0.74      0.79       106\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.80      0.80      0.79       200\n",
      "weighted avg       0.80      0.80      0.79       200\n",
      "\n",
      "[[81 13]\n",
      " [28 78]]\n"
     ]
    }
   ],
   "source": [
    "#3.Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "tree.fit(x_train,y_train.values.ravel())\n",
    "predict_tree = tree.predict(x_test)\n",
    "print(\"Decision Tree:\",accuracy_score(y_test,predict_tree))\n",
    "print(metrics.classification_report(y_test,predict_tree))\n",
    "print(metrics.confusion_matrix(y_test,predict_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83        94\n",
      "           1       0.84      0.86      0.85       106\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.84      0.84      0.84       200\n",
      "weighted avg       0.84      0.84      0.84       200\n",
      "\n",
      "[[77 17]\n",
      " [15 91]]\n"
     ]
    }
   ],
   "source": [
    "#4.Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=5)\n",
    "rf.fit(x_train,y_train.values.ravel())\n",
    "predict_rf = rf.predict(x_test)\n",
    "print(\"Random Forest:\",accuracy_score(y_test,predict_rf))\n",
    "print(metrics.classification_report(y_test,predict_rf))\n",
    "print(metrics.confusion_matrix(y_test,predict_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90        94\n",
      "           1       0.91      0.92      0.91       106\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.90      0.90      0.90       200\n",
      "weighted avg       0.90      0.91      0.90       200\n",
      "\n",
      "[[84 10]\n",
      " [ 9 97]]\n"
     ]
    }
   ],
   "source": [
    "#5.SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(x_train,y_train.values.ravel())\n",
    "predict_svm = svm.predict(x_test)\n",
    "print(\"SVM:\",accuracy_score(y_test,predict_svm))\n",
    "print(metrics.classification_report(y_test,predict_svm))\n",
    "print(metrics.confusion_matrix(y_test,predict_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.81\n",
      "Logistic Regression: 0.835\n",
      "Nearest Neighbors: 0.91\n",
      "Decision Tree: 0.795\n",
      "Random Forest: 0.84\n",
      "SVM: 0.905\n"
     ]
    }
   ],
   "source": [
    "#All in One place\n",
    "print(\"Naive Bayes:\",accuracy_score(y_test,predict_gnb))\n",
    "print(\"Logistic Regression:\",accuracy_score(y_test,predict_lr))\n",
    "print(\"Nearest Neighbors:\",accuracy_score(y_test,predict_knn))\n",
    "print(\"Decision Tree:\",accuracy_score(y_test,predict_tree))\n",
    "print(\"Random Forest:\",accuracy_score(y_test,predict_rf))\n",
    "print(\"SVM:\",accuracy_score(y_test,predict_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "norm = Normalizer()\n",
    "x_norm_train = norm.fit_transform(x_train)\n",
    "\n",
    "x_norm_test = norm.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.805\n",
      "Logistic Regression: 0.83\n",
      "Nearest Neighbors: 0.89\n",
      "Decision Tree: 0.81\n",
      "Random Forest: 0.85\n",
      "SVM: 0.78\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_norm_train,y_train)\n",
    "predict_gnb = gnb.predict(x_norm_test)\n",
    "print(\"Naive Bayes:\",accuracy_score(y_test,predict_gnb))\n",
    "\n",
    "\n",
    "#2.Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_norm_train,y_train)\n",
    "predict_lr = lr.predict(x_norm_test)\n",
    "print(\"Logistic Regression:\",accuracy_score(y_test,predict_lr))\n",
    "\n",
    "\n",
    "#3.K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2, weights='uniform', algorithm='auto',\n",
    "                           leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn.fit(x_norm_train,y_train)\n",
    "predict_knn = knn.predict(x_norm_test)\n",
    "print(\"Nearest Neighbors:\",accuracy_score(y_test,predict_knn))\n",
    "\n",
    "\n",
    "#3.Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "tree.fit(x_norm_train,y_train)\n",
    "predict_tree = tree.predict(x_norm_test)\n",
    "print(\"Decision Tree:\",accuracy_score(y_test,predict_tree))\n",
    "\n",
    "#4.Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=5)\n",
    "rf.fit(x_norm_train,y_train)\n",
    "predict_rf = rf.predict(x_norm_test)\n",
    "print(\"Random Forest:\",accuracy_score(y_test,predict_rf))\n",
    "\n",
    "#4.SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(x_norm_train,y_train)\n",
    "predict_svm = svm.predict(x_test)\n",
    "print(\"SVM:\",accuracy_score(y_test,predict_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6)\n",
    "x_pca_train = pca.fit_transform(x_train)\n",
    "\n",
    "x_pca_test = pca.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.745\n",
      "Logistic Regression: 0.785\n",
      "Nearest Neighbors: 0.67\n",
      "Decision Tree: 0.69\n",
      "Random Forest: 0.75\n",
      "SVM: 0.71\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_pca_train,y_train)\n",
    "predict_gnb = gnb.predict(x_pca_test)\n",
    "print(\"Naive Bayes:\",accuracy_score(y_test,predict_gnb))\n",
    "\n",
    "\n",
    "#2.Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_pca_train,y_train)\n",
    "predict_lr = lr.predict(x_pca_test)\n",
    "print(\"Logistic Regression:\",accuracy_score(y_test,predict_lr))\n",
    "\n",
    "\n",
    "#3.K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2, weights='uniform', algorithm='auto',\n",
    "                           leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn.fit(x_pca_train,y_train)\n",
    "predict_knn = knn.predict(x_pca_test)\n",
    "print(\"Nearest Neighbors:\",accuracy_score(y_test,predict_knn))\n",
    "\n",
    "\n",
    "#3.Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "tree.fit(x_pca_train,y_train)\n",
    "predict_tree = tree.predict(x_pca_test)\n",
    "print(\"Decision Tree:\",accuracy_score(y_test,predict_tree))\n",
    "\n",
    "#4.Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=5)\n",
    "rf.fit(x_pca_train,y_train)\n",
    "predict_rf = rf.predict(x_pca_test)\n",
    "print(\"Random Forest:\",accuracy_score(y_test,predict_rf))\n",
    "\n",
    "#4.SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(x_pca_train,y_train)\n",
    "predict_svm = svm.predict(x_pca_test)\n",
    "print(\"SVM:\",accuracy_score(y_test,predict_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_all shape : (10000, 40)\n",
      "Random Forest Best Score 0.996\n",
      "Random Forest Best Parmas {'max_depth': 3, 'n_estimators': 10}\n",
      "Random Forest Accuracy 0.9960000000000001\n",
      "KNN Best Score 0.996\n",
      "KNN Best Params {'n_neighbors': 3}\n",
      "KNN Accuracy 0.9960000000000001\n",
      "SVM Best Score 0.996\n",
      "SVM Best Params {'C': 1, 'kernel': 'linear'}\n",
      "SVM Accuracy 0.9960000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "x_all = np.r_[train,test]\n",
    "print('x_all shape :',x_all.shape)\n",
    "\n",
    "# USING THE GAUSSIAN MIXTURE MODEL \n",
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n",
    "        gmm.fit(x_all)\n",
    "        bic.append(gmm.aic(x_all))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "            \n",
    "best_gmm.fit(x_all)\n",
    "gmm_train = best_gmm.predict_proba(train)\n",
    "gmm_test = best_gmm.predict_proba(test)\n",
    "\n",
    "\n",
    "#Random Forest Classifier\n",
    "rfc = RandomForestClassifier(random_state=99)\n",
    "\n",
    "#USING GRID SEARCH\n",
    "n_estimators = [10, 50, 100, 200,400]\n",
    "max_depth = [3, 10, 20, 40]\n",
    "param_grid = dict(n_estimators=n_estimators,max_depth=max_depth)\n",
    "\n",
    "grid_search_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = 10,scoring='accuracy',n_jobs=-1).fit(gmm_train, trainLabels.values.ravel())\n",
    "rfc_best = grid_search_rfc.best_estimator_\n",
    "print('Random Forest Best Score',grid_search_rfc.best_score_)\n",
    "print('Random Forest Best Parmas',grid_search_rfc.best_params_)\n",
    "print('Random Forest Accuracy',cross_val_score(rfc_best,gmm_train, trainLabels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#KNN \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#USING GRID SEARCH\n",
    "n_neighbors=[3,5,6,7,8,9,10]\n",
    "param_grid = dict(n_neighbors=n_neighbors)\n",
    "\n",
    "grid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train,trainLabels.values.ravel())\n",
    "knn_best = grid_search_knn.best_estimator_\n",
    "print('KNN Best Score', grid_search_knn.best_score_)\n",
    "print('KNN Best Params',grid_search_knn.best_params_)\n",
    "print('KNN Accuracy',cross_val_score(knn_best,gmm_train, trainLabels.values.ravel(), cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "svc = SVC()\n",
    "\n",
    "#USING GRID SEARCH\n",
    "parameters = [{'kernel':['linear'],'C':[1,10,100]},\n",
    "              {'kernel':['rbf'],'C':[1,10,100],'gamma':[0.05,0.0001,0.01,0.001]}]\n",
    "grid_search_svm = GridSearchCV(estimator=svc, param_grid=parameters, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train, trainLabels.values.ravel())\n",
    "svm_best = grid_search_svm.best_estimator_\n",
    "print('SVM Best Score',grid_search_svm.best_score_)\n",
    "print('SVM Best Params',grid_search_svm.best_params_)\n",
    "print('SVM Accuracy',cross_val_score(svm_best,gmm_train, trainLabels.values.ravel(), cv=10).mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
